# NLP Deep
## Authors
- Luu Hoang Long Vo (luu-hoang-long.vo@epita.fr)
- Yassin Bouhassoun (yassin.bouhassoun@epita.fr)
- Phu Hien Le (phu-hien.le@epita.fr)
- Youssef Bouarfa Dinia (youssef.bouarfa-dinia@epita.fr)

## Architecture

```bash
├── README.md
├── lab_05
│   └── nlp05.ipynb
├── lab_06
│   └── nlp06.ipynb
├── lab_07
│   └── nlp07.ipynb
```
- In each `lab_*` folder there will be a Jupyter Notebook that contains the code + report of the objectives. All the questions are answered within the jupyter notebook using markdown to provide structure.

## Lab 05

### Short description of the project

- The lab is an introduction to language translation using Transformer. We perform language translation on 3 different decoders (Top-K Sampling, Greedy, Beam Search) and compare their performances based the BLEU score.

## Lab 06

### Short description of the project

- The project is about fine-tuning a pre-trained model to perform text classification on the IMDB library dataset. In our report, we use distilbert as the pre-trained model to fine-tune.

## Lab 07

### Short description of the project

- The project's goal is to evaluate a semantic search with and without nearest neighbours approximation based on the Mean Average Precision and runtime

## Lab 08

### Short description of the project

- The project's goal is to analyze dataset and the pretrained model RoBERTa's performance, the project also shows how to properly annotate the dataset and how to evaluate the inter-annotator agreement using Fleiss Kappa
